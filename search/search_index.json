{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#machine-learning","title":"Machine Learning","text":"<p>P\u00e1gina inicial do projeto de Machine Learning do Grupo 4 do curso de Machine Learning da ESPM.</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de aprendizado de m\u00e1quina para resolver problemas reais e fornecer insights valiosos a partir dos dados analisados.</p>"},{"location":"classes/decision_tree/","title":"Projeto","text":""},{"location":"classes/decision_tree/#projeto","title":"Projeto","text":""},{"location":"classes/decision_tree/#objetivo","title":"Objetivo","text":"<p>Nosso objetivo \u00e9 entender o por qu\u00ea os funcion\u00e1rios deixam a empresa, e tentar prever quais funcion\u00e1rios est\u00e3o propensos a sair, para que medidas possam ser tomadas para evitar a rotatividade.</p> <p>Hip\u00f3tese: Funcion\u00e1rios que est\u00e3o insatisfeitos com o ambiente de trabalho, sal\u00e1rio ou oportunidades de crescimento s\u00e3o mais propensos a deixar a empresa.</p>"},{"location":"classes/decision_tree/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Os dados foram coletados de uma empresa fict\u00edcia e incluem informa\u00e7\u00f5es sobre os funcion\u00e1rios, como idade, sal\u00e1rio, satisfa\u00e7\u00e3o no trabalho, entre outros. A vari\u00e1vel alvo \u00e9 \"Attrition\", que indica se o funcion\u00e1rio deixou a empresa (Yes) ou n\u00e3o (No).</p> <p>A partir desta an\u00e1lise inicial, podemos perceber que n\u00e3o temos valores nulos e que a maioria das vari\u00e1veis s\u00e3o num\u00e9ricas, o que facilita a aplica\u00e7\u00e3o de algoritmos de machine learning. Por\u00e9m, teremos que tratar as vari\u00e1veis categ\u00f3ricas com as t\u00e9cnicas de encoding.</p> <p>== Output</p> <pre><code>``` \n\n```\n</code></pre>"},{"location":"classes/kmeans/","title":"Projeto","text":"<p>K-Means Clustering is an unsupervised machine learning algorithm used to partition a dataset into \\( K \\) distinct, non-overlapping clusters. The algorithm assigns each data point to the cluster with the nearest centroid (mean) based on a distance metric, typically Euclidean distance. It is widely used in data analysis, pattern recognition, and image processing due to its simplicity and efficiency.</p>"},{"location":"classes/kmeans/#key-concepts","title":"Key Concepts","text":"<ul> <li>Clusters: Groups of data points that are similar to each other based on a distance metric.</li> <li>Unsupervised Learning: The algorithm works without labeled data, identifying patterns based solely on the data's structure.</li> <li>Centroids: These are the \"centers\" of the clusters, represented as the mean (average) of all points in a cluster.</li> <li>K: The number of clusters, which must be specified in advance (e.g., K=3 means dividing data into 3 clusters).</li> <li>Distance Metric: Typically, Euclidean distance is used to measure how far a data point is from a centroid. The goal is to assign points to the nearest centroid.</li> <li> <p>Objective: Minimize the within-cluster sum of squares (WCSS), which is the sum of squared distances between each point and its assigned centroid. Mathematically, for a dataset \\( X = \\{x_1, x_2, \\dots, x_n\\} \\) and centroids \\( \\mu = \\{\\mu_1, \\mu_2, \\dots, \\mu_K\\} \\), the objective is:</p> \\[ \\arg\\min_{\\mu} \\sum_{i=1}^K \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 \\] <p>where \\( C_i \\) is the set of points in cluster \\( i \\).</p> </li> </ul> <p>K-Means assumes clusters are spherical and equally sized, which may not always hold for real data.</p>"},{"location":"classes/kmeans/#algorithm-step-by-step","title":"Algorithm: Step-by-Step","text":"<p>The K-Means algorithm is iterative and consists of the following steps:</p> <ol> <li> <p>Initialization:</p> <ul> <li>Choose the value of K (number of clusters).</li> <li>Randomly select K initial centroids from the dataset. (A common improvement is K-Means++ initialization, which spreads out the initial centroids to avoid poor starting points.)</li> </ul> </li> <li> <p>Assignment Step (Expectation):</p> <ul> <li>For each data point in the dataset, calculate its distance to all K centroids.</li> <li>Assign the point to the cluster with the closest centroid (using Euclidean distance or another metric).</li> <li>This creates K clusters, where each point belongs to exactly one cluster.</li> </ul> </li> <li> <p>Update Step (Maximization):</p> <ul> <li>For each cluster, recalculate the centroid as the mean (average) of all points assigned to that cluster.</li> <li>Update the centroids with these new values.</li> </ul> </li> <li> <p>Iteration:</p> <ul> <li>Repeat steps 2 and 3 until one of the stopping criteria is met:<ul> <li>Centroids no longer change (or change by less than a small threshold, e.g., 0.001).</li> <li>A maximum number of iterations is reached (to prevent infinite loops).</li> <li>The WCSS decreases minimally between iterations.</li> </ul> </li> </ul> </li> <li> <p>Output:</p> <ul> <li>The final centroids and the cluster assignments for each data point.</li> </ul> </li> </ol> <p>The algorithm converges because the WCSS is non-increasing with each iteration, but it may converge to a local optimum (not always the global best). Running it multiple times with different initializations helps mitigate this.</p>"},{"location":"classes/kmeans/#example-1","title":"Example 1","text":"<p>Suppose you have a 2D dataset with 5 points: (1,2), (2,1), (5,8), (6,7), (8,6). Let K=2.</p> <ul> <li>Initialization: Randomly pick centroids, say C1=(1,2) and C2=(5,8).</li> <li>Assignment:<ul> <li>(1,2) and (2,1) are closer to C1 \u2192 Cluster 1.</li> <li>(5,8), (6,7), (8,6) are closer to C2 \u2192 Cluster 2.</li> </ul> </li> <li>Update:<ul> <li>New C1 = average of (1,2) and (2,1) = (1.5, 1.5).</li> <li>New C2 = average of (5,8), (6,7), (8,6) = (6.33, 7).</li> </ul> </li> <li>Repeat: Reassign points based on new centroids. This continues until stable.</li> </ul> <p>After convergence, you might end up with two clusters: one around (1.5,1.5) and one around (6.33,7).</p>"},{"location":"classes/kmeans/#example-2","title":"Example 2","text":"ResultCode 2025-09-30T23:43:24.721407 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.cluster import KMeans\n\nplt.figure(figsize=(12, 10))\n\n# Generate sample data\nnp.random.seed(42)\nX = np.concatenate([\n    np.random.normal(0, 1, (100, 2)),\n    np.random.normal(5, 1, (100, 2)),\n    np.random.normal(10, 1, (100, 2))\n])\n\n# Run K-Means\nkmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# Plot results\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           c='red', marker='*', s=200, label='Centroids')\nplt.title('K-Means Clustering Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\n\n# # Print centroids and inertia\n# print(\"Final centroids:\", kmeans.cluster_centers_)\n# print(\"Inertia (WCSS):\", kmeans.inertia_)\n\n# # Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"classes/kmeans/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":"Aspect Advantages Disadvantages Simplicity Easy to understand and implement; computationally efficient (O(n) per iteration). Sensitive to initial centroid placement; may converge to local optima. Scalability Works well on large datasets with linear time complexity. Assumes spherical clusters; struggles with non-convex or varying densities. Output Produces tight, compact clusters; interpretable centroids. Requires predefined K; outliers can skew results."},{"location":"classes/kmeans/#choosing-k","title":"Choosing K","text":"<ul> <li>Elbow Method: Plot WCSS vs. K and look for the \"elbow\" where the rate of decrease slows (e.g., K=3 if the curve bends sharply there).</li> <li>Silhouette Score: Measures how similar points are within their cluster vs. other clusters (higher is better, range -1 to 1).</li> <li>Other methods: Gap statistic or domain knowledge.</li> </ul>"},{"location":"classes/kmeans/#implementation-tip","title":"Implementation Tip","text":"<p>In Python, you can use scikit-learn's <code>KMeans</code> class: </p><pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\nX = np.array([[1,2], [2,1], [5,8], [6,7], [8,6]])  # Your data\nkmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\nkmeans.fit(X)\nprint(kmeans.labels_)  # Cluster assignments: e.g., [0, 0, 1, 1, 1]\nprint(kmeans.cluster_centers_)  # Centroids\n</code></pre><p></p> <p>K-Means is a foundational algorithm, but variants like hierarchical clustering or DBSCAN may be better for certain data types. If you have specific data or code to run, let me know for a demo!</p>"},{"location":"classes/kmeans/#additional","title":"Additional","text":""},{"location":"classes/kmeans/#k-means-initialization-explanation","title":"K-Means++ Initialization: Explanation","text":"<p>K-Means++ is an improved initialization method for the K-Means clustering algorithm, designed to address the sensitivity of standard K-Means to the initial placement of centroids. Randomly choosing initial centroids in standard K-Means can lead to poor clustering results or convergence to suboptimal local minima. K-Means++ mitigates this by strategically selecting initial centroids to be spread out across the data, improving both the quality of clusters and convergence speed.</p>"},{"location":"classes/kmeans/#why-k-means","title":"Why K-Means++?","text":"<p>In standard K-Means, centroids are often initialized randomly, which can result in:</p> <ul> <li>Poor clustering: Random centroids might be too close to each other, leading to unbalanced or suboptimal clusters.</li> <li>Slow convergence: Bad initial placements require more iterations to reach a stable solution.</li> <li>Inconsistent results: Different runs produce varying clusters due to random initialization.</li> </ul> <p>K-Means++ addresses these issues by choosing initial centroids in a way that maximizes their separation, reducing the likelihood of poor starting conditions.</p>"},{"location":"classes/kmeans/#k-means-initialization","title":"K-Means++ Initialization","text":"<p>The K-Means++ algorithm selects the initial K centroids iteratively, using a probabilistic approach that favors points farther from already chosen centroids. Here\u2019s the step-by-step process for a dataset \\( X = \\{x_1, x_2, \\dots, x_n\\} \\) and \\( K \\) clusters:</p> <ol> <li> <p>First Centroid:</p> <ul> <li>Randomly select one data point from the dataset as the first centroid \\( \\mu_1 \\). This is typically done uniformly at random to ensure fairness.</li> </ul> </li> <li> <p>Subsequent Centroids:</p> <ul> <li>For each remaining centroid (from 2 to K):<ul> <li>Compute the squared Euclidean distance \\( D(x) \\) from each data point \\( x \\) to the nearest already-selected centroid.</li> <li>Assign a probability to each point \\( x \\): \\( \\frac{D(x)^2}{\\sum_{x' \\in X} D(x')^2} \\). Points farther from existing centroids have a higher probability of being chosen.</li> <li>Select the next centroid by sampling a point from the dataset, weighted by these probabilities.</li> </ul> </li> <li>This ensures new centroids are likely to be far from existing ones, spreading them across the data.</li> </ul> </li> <li> <p>Repeat:</p> <ul> <li>Continue selecting centroids until all K are chosen.</li> </ul> </li> <li> <p>Proceed to K-Means:</p> <ul> <li>Use these K centroids as the starting point for the standard K-Means algorithm (assign points to nearest centroids, update centroids, iterate until convergence).</li> </ul> </li> </ol>"},{"location":"classes/kmeans/#mathematical-intuition","title":"Mathematical Intuition","text":"<p>The probability function \\( \\frac{D(x)^2}{\\sum D(x')^2} \\) uses squared distances to emphasize points that are farther away. This creates a \"repulsive\" effect, where new centroids are more likely to be placed in regions of the dataset that are not yet covered by existing centroids. The result is a set of initial centroids that are well-distributed, reducing the chance of clustering points into suboptimal groups.</p> <p>The expected approximation ratio of K-Means++ is \\( O(\\log K) \\)-competitive with the optimal clustering, a significant improvement over random initialization, which has no such guarantee.</p>"},{"location":"classes/kmeans/#example","title":"Example","text":"<p>Suppose you have a dataset with points: (1,1), (2,2), (8,8), (9,9), and you want \\( K=2 \\):</p> <ul> <li>Step 1: Randomly pick (1,1) as the first centroid.</li> <li> <p>Step 2: Calculate squared distances to (1,1):</p> <ul> <li>(1,1): \\( 0^2 = 0 \\)</li> <li>(2,2): \\( (1^2 + 1^2) = 2 \\)</li> <li>(8,8): \\( (7^2 + 7^2) = 98 \\)</li> <li>(9,9): \\( (8^2 + 8^2) = 128 \\)</li> <li>Total: \\( 0 + 2 + 98 + 128 = 228 \\).</li> <li> <p>Probabilities:</p> <p>(1,1): \\( 0/228 = 0 \\),</p> <p>(2,2): \\( 2/228 \\approx 0.009 \\),</p> <p>(8,8): \\( 98/228 \\approx 0.43 \\),</p> <p>(9,9): \\( 128/228 \\approx 0.56 \\).</p> </li> <li> <p>Likely pick (9,9) or (8,8) as the second centroid due to their high probabilities (far from (1,1)).</p> </li> </ul> </li> <li> <p>Result: Centroids like (1,1) and (9,9) are well-spread, leading to better clustering than if (1,1) and (2,2) were chosen.</p> </li> </ul>"},{"location":"classes/kmeans/#advantages-and-disadvantages_1","title":"Advantages and Disadvantages","text":"Aspect Advantages Disadvantages Quality Produces better initial centroids, leading to lower WCSS and better clusters. Slightly more computationally expensive than random initialization. Convergence Often converges faster due to better starting points (fewer iterations). Still requires predefined K; sensitive to outliers (can skew distances). Consistency More consistent results across runs compared to random initialization. Random first centroid can still introduce some variability."},{"location":"classes/kmeans/#computational-cost","title":"Computational Cost","text":"<ul> <li>Random Initialization: O(K) for picking K random points.</li> <li>K-Means++: O(nK) for computing distances to select K centroids, where n is the number of points. This is a small overhead compared to the K-Means iterations (O(nKI), where I is the number of iterations), and the improved clustering quality often outweighs the cost.</li> </ul>"},{"location":"classes/kmeans/#implementation","title":"Implementation","text":"<p>In Python\u2019s scikit-learn, K-Means++ is the default initialization method for the <code>KMeans</code> class: </p><pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\nX = np.array([[1,1], [2,2], [8,8], [9,9]])\nkmeans = KMeans(n_clusters=2, init='k-means++', random_state=42, n_init=10)\nkmeans.fit(X)\nprint(kmeans.labels_)  # Cluster assignments\nprint(kmeans.cluster_centers_)  # Centroids\n</code></pre> The <code>init='k-means++'</code> parameter explicitly sets K-Means++ initialization (though it\u2019s default in scikit-learn).<p></p>"},{"location":"classes/kmeans/#practical-notes","title":"Practical Notes","text":"<ul> <li>Choosing K: K-Means++ still requires you to specify K. Use methods like the elbow method or silhouette score to determine an optimal K.</li> <li>Outliers: Outliers can disproportionately affect centroid selection due to squared distances. Preprocessing (e.g., removing outliers) can help.</li> <li>Scalability: For very large datasets, variants like scalable K-Means++ or mini-batch K-Means can be used to reduce computational cost.</li> </ul> <p>K-Means++ is a robust improvement over random initialization, widely used in practice due to its balance of simplicity and effectiveness. If you have a dataset or want a visual demo of K-Means++ vs. random initialization, let me know!</p>"},{"location":"classes/kmeans/#additional_1","title":"Additional","text":""},{"location":"classes/kmeans/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p>:calendar: 21.sep :clock3: 23:59</p> <p>:material-account: Individual</p> <p>:simple-target: Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de K-Means para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio K-Means. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo KNN. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20"},{"location":"classes/knn/","title":"Projeto","text":"<p>K-Nearest Neighbors (KNN) is a simple, versatile, and non-parametric machine learning algorithm used for classification and regression tasks. It operates on the principle of similarity, predicting the label or value of a data point based on the majority class or average of its k nearest neighbors in the feature space. KNN is intuitive and effective for small datasets or when interpretability is key.</p>"},{"location":"classes/knn/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>Instance-Based Learning: KNN is a lazy learning algorithm, meaning no explicit training phase is required. It stores the entire dataset and performs calculations at prediction time.</p> </li> <li> <p>Distance Metric: The algorithm measures the distance between data points to identify the nearest neighbors. Common metrics include:</p> Metric Formula Euclidean distance \\( \\displaystyle \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} \\) Manhattan distance \\( \\displaystyle \\sum_{i=1}^n \\|x_i - y_i\\| \\) Minkowski distance \\( \\displaystyle \\left( \\sum_{i=1}^n \\|x_i - y_i\\|^p \\right)^{1/p} \\) </li> <li> <p>K Value: The number of neighbors considered. A small k can be sensitive to noise, while a large k smooths predictions but may dilute patterns.</p> </li> <li> <p>Decision Rule:</p> <ul> <li>Classification: The majority class among the k neighbors determines the predicted class.</li> <li>Regression: The average (or weighted average) of the k neighbors' values is used.</li> </ul> </li> </ul>"},{"location":"classes/knn/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>KNN relies on distance calculations to find neighbors. For a data point \\( x \\), the algorithm: 1. Computes the distance to all points in the dataset using a chosen metric (e.g., Euclidean distance). 2. Selects the k closest points. 3. For classification, assigns the class with the most votes among the k neighbors. For regression, computes the mean of their values.</p>"},{"location":"classes/knn/#example-classification","title":"Example: Classification","text":"<p>Given a dataset \\( D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\} \\), where \\( x_i \\) is a feature vector and \\( y_i \\) is the class label, predict the class of a new point \\( x \\):</p> <ul> <li>Calculate distances \\( d(x, x_i) \\) for all \\( i \\).</li> <li>Sort distances and select the k smallest.</li> <li>Count the class labels of these k points and assign the majority class to \\( x \\).</li> </ul>"},{"location":"classes/knn/#weighted-knn","title":"Weighted KNN","text":"<p>In weighted KNN, neighbors contribute to the prediction based on their distance. Closer neighbors have higher influence, often weighted by the inverse of their distance:</p> \\[ w_i = \\frac{1}{d(x, x_i)} \\] <p>For regression, the prediction is:</p> \\[ \\hat{y} = \\frac{\\sum_{i=1}^k w_i y_i}{\\sum_{i=1}^k w_i} \\]"},{"location":"classes/knn/#visualizing-knn","title":"Visualizing KNN","text":"<p>To illustrate, consider a 2D dataset with two classes (blue circles and red triangles). For a new point (green star), KNN identifies the k nearest points and assigns the majority class.</p> <p></p> <p>Figure: KNN with k=3. The green star is classified based on the majority class (blue circles) among its three nearest neighbors.</p> <p>For regression, imagine predicting a continuous value (e.g., house price) based on the average of the k nearest houses\u2019 prices.</p>"},{"location":"classes/knn/#plot-decision-boundary","title":"Plot: Decision Boundary","text":"<p>KNN\u2019s decision boundary is non-linear and depends on the data distribution. Below is an example of decision boundaries for different k values:</p> <p></p> <p>Figure: Decision boundaries for k=1, k=5, and k=15. Smaller k leads to more complex boundaries, while larger k smooths them.</p>"},{"location":"classes/knn/#pros-and-cons-of-knn","title":"Pros and Cons of KNN","text":""},{"location":"classes/knn/#pros","title":"Pros","text":"<ul> <li>Simplicity: Easy to understand and implement.</li> <li>Non-Parametric: Makes no assumptions about data distribution, suitable for non-linear data.</li> <li>Versatility: Works for both classification and regression.</li> <li>Adaptability: Can handle multi-class problems and varying data types with appropriate distance metrics.</li> </ul>"},{"location":"classes/knn/#cons","title":"Cons","text":"<ul> <li>Computational Cost: Slow for large datasets, as it requires calculating distances for every prediction.</li> <li>Memory Intensive: Stores the entire dataset, which can be problematic for big data.</li> <li>Sensitive to Noise: Outliers or irrelevant features can degrade performance.</li> <li>Curse of Dimensionality: Performance drops in high-dimensional spaces due to sparse data.</li> <li>Choosing K: Requires careful tuning of k and distance metric to balance bias and variance.</li> </ul>"},{"location":"classes/knn/#knn-implementation","title":"KNN Implementation","text":"<p>Below are two implementations of KNN: one from scratch and one using Python\u2019s scikit-learn library.</p>"},{"location":"classes/knn/#from-scratch","title":"From Scratch","text":"<p>This implementation includes a basic KNN classifier using Euclidean distance.</p> ResultCode <p>Accuracy: 1.00 </p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass KNNClassifier:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        predictions = [self._predict(x) for x in X]\n        return np.array(predictions)\n\n    def _predict(self, x):\n        # Compute Euclidean distances\n        distances = [np.sqrt(np.sum((x - x_train)**2)) for x_train in self.X_train]\n        # Get indices of k-nearest neighbors\n        k_indices = np.argsort(distances)[:self.k]\n        # Get corresponding labels\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n        # Return majority class\n        most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n        return most_common\n\n# Example usage\n\n# Generate synthetic dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train and predict\nknn = KNNClassifier(k=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n</code></pre>"},{"location":"classes/knn/#using-scikit-learn","title":"Using Scikit-Learn","text":"ResultCode <p>Accuracy: 1.00  2025-09-30T23:43:25.181872 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ </p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\n\nplt.figure(figsize=(12, 10))\n\n# Generate synthetic dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train KNN model\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n\n# Visualize decision boundary\nh = 0.02  # Step size in mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.3)\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, style=y, palette=\"deep\", s=100)\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"KNN Decision Boundary (k=3)\")\n\n# Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"classes/knn/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p>:calendar: 16.sep :clock3: 23:59</p> <p>:material-account: Individual</p> <p>:simple-target: Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de KNN para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio KNN. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo KNN. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20"}]}